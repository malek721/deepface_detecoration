{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T09:43:13.029236Z",
     "start_time": "2025-04-19T09:42:57.527269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ðŸ”§ Import libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "2b912f65d30822a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T09:43:23.620310Z",
     "start_time": "2025-04-19T09:43:13.060918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ðŸ“‚ Prepare the dataset (update path if needed)\n",
    "data_dir = r'C:\\Users\\admin\\Desktop\\yapa zeka\\deepfake_split_dataset'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "f1c9417d4e6697cd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-19T10:02:51.355427Z",
     "start_time": "2025-04-19T10:02:51.335306Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A deep residual CNN for deepfake detection, encapsulated in a single MyCNN class.\n",
    "    \"\"\"\n",
    "    class ResidualBlock(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, stride=1, downsample=None, drop_rate=0.3):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                                   stride=stride, padding=1, bias=False)\n",
    "            self.bn1   = nn.BatchNorm2d(out_channels)\n",
    "            self.relu  = nn.ReLU(inplace=True)\n",
    "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                                   stride=1, padding=1, bias=False)\n",
    "            self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "            self.drop  = nn.Dropout(drop_rate)\n",
    "            self.downsample = downsample\n",
    "\n",
    "        def forward(self, x):\n",
    "            identity = x\n",
    "\n",
    "            out = self.conv1(x)\n",
    "            out = self.bn1(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.drop(out)\n",
    "            out = self.conv2(out)\n",
    "            out = self.bn2(out)\n",
    "\n",
    "            if self.downsample is not None:\n",
    "                identity = self.downsample(x)\n",
    "\n",
    "            out += identity\n",
    "            out = self.relu(out)\n",
    "            return out\n",
    "\n",
    "    def __init__(self, num_classes=2, drop_rate=0.3):\n",
    "        super().__init__()\n",
    "        # Stem: initial convolution and pooling\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        )\n",
    "        # Residual stages\n",
    "        self.layer1 = self._make_layer(64,  64,  blocks=2, stride=1,  drop_rate=drop_rate)\n",
    "        self.layer2 = self._make_layer(64, 128, blocks=2, stride=2,  drop_rate=drop_rate)\n",
    "        self.layer3 = self._make_layer(128,256, blocks=2, stride=2,  drop_rate=drop_rate)\n",
    "        self.layer4 = self._make_layer(256,512, blocks=2, stride=2,  drop_rate=drop_rate)\n",
    "\n",
    "        # Head: pooling and classification\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride, drop_rate):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        layers = [MyCNN.ResidualBlock(in_channels, out_channels, stride, downsample, drop_rate)]\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(MyCNN.ResidualBlock(out_channels, out_channels, drop_rate=drop_rate))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T11:34:56.786789Z",
     "start_time": "2025-04-19T10:02:54.940378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm  # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒØªØ¨Ø© tqdm\n",
    "\n",
    "# ðŸš€ Train the model\n",
    "model = MyCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 15  # Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ 5 epochs\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train, total_train = 0, 0\n",
    "    \n",
    "    # Ø§Ø³ØªØ®Ø¯Ø§Ù… tqdm Ù„Ø¹Ø±Ø¶ Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù… Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
    "    with tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}', unit='batch', ncols=100) as pbar:\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "            \n",
    "            # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø®Ø³Ø§Ø±Ø©\n",
    "            train_accuracy = 100 * correct_train / total_train\n",
    "            pbar.set_postfix(train_loss=running_loss / (total_train // len(labels)), train_acc=f'{train_accuracy:.4f}%')\n",
    "    \n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    # Ø§Ø³ØªØ®Ø¯Ø§Ù… tqdm Ù„Ø¹Ø±Ø¶ Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù… Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚\n",
    "    with tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{num_epochs}', unit='batch', ncols=100) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for images, labels in pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© Ù„Ù„ØªØ­Ù‚Ù‚\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù… Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø®Ø³Ø§Ø±Ø©\n",
    "                val_accuracy = 100 * correct / total\n",
    "                pbar.set_postfix(val_loss=val_loss / (total // len(labels)), val_acc=f'{val_accuracy:.4f}%')\n",
    "    \n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_losses[-1]:.4f}, Val Acc: {acc:.2f}%\")\n"
   ],
   "id": "dd108aa89720518e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/15: 100%|â–ˆ| 4218/4218 [05:18<00:00, 13.26batch/s, train_acc=86.5563%, train_loss=0.\n",
      "Validation Epoch 1/15: 100%|â–ˆ| 528/528 [00:37<00:00, 14.03batch/s, val_acc=96.4552%, val_loss=0.0162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Train Loss: 0.2889, Val Acc: 96.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/15: 100%|â–ˆ| 4218/4218 [05:06<00:00, 13.74batch/s, train_acc=96.2128%, train_loss=0.\n",
      "Validation Epoch 2/15: 100%|â–ˆ| 528/528 [00:36<00:00, 14.55batch/s, val_acc=97.7593%, val_loss=0.0090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Train Loss: 0.0932, Val Acc: 97.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/15: 100%|â–ˆ| 4218/4218 [05:11<00:00, 13.54batch/s, train_acc=97.3732%, train_loss=0.\n",
      "Validation Epoch 3/15: 100%|â–ˆ| 528/528 [00:36<00:00, 14.47batch/s, val_acc=97.8601%, val_loss=0.0093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Train Loss: 0.0596, Val Acc: 97.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/15: 100%|â–ˆ| 4218/4218 [05:13<00:00, 13.46batch/s, train_acc=97.7630%, train_loss=0.\n",
      "Validation Epoch 4/15: 100%|â–ˆ| 528/528 [00:36<00:00, 14.47batch/s, val_acc=98.0439%, val_loss=0.0067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Train Loss: 0.0475, Val Acc: 98.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/15: 100%|â–ˆ| 4218/4218 [05:14<00:00, 13.42batch/s, train_acc=98.0386%, train_loss=0.\n",
      "Validation Epoch 5/15: 100%|â–ˆ| 528/528 [00:36<00:00, 14.42batch/s, val_acc=98.1624%, val_loss=0.0062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Train Loss: 0.0409, Val Acc: 98.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/15: 100%|â–ˆ| 4218/4218 [05:14<00:00, 13.42batch/s, train_acc=98.1416%, train_loss=0.\n",
      "Validation Epoch 6/15: 100%|â–ˆ| 528/528 [00:36<00:00, 14.36batch/s, val_acc=98.4055%, val_loss=0.0057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Train Loss: 0.0369, Val Acc: 98.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/15: 100%|â–ˆ| 4218/4218 [05:28<00:00, 12.86batch/s, train_acc=98.2357%, train_loss=0.\n",
      "Validation Epoch 7/15: 100%|â–ˆ| 528/528 [00:43<00:00, 12.04batch/s, val_acc=98.1802%, val_loss=0.0063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Train Loss: 0.0332, Val Acc: 98.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/15: 100%|â–ˆ| 4218/4218 [05:16<00:00, 13.32batch/s, train_acc=98.3194%, train_loss=0.\n",
      "Validation Epoch 8/15: 100%|â–ˆ| 528/528 [00:35<00:00, 15.02batch/s, val_acc=98.3817%, val_loss=0.0052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Train Loss: 0.0320, Val Acc: 98.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/15: 100%|â–ˆ| 4218/4218 [05:10<00:00, 13.60batch/s, train_acc=98.3639%, train_loss=0.\n",
      "Validation Epoch 9/15: 100%|â–ˆ| 528/528 [00:36<00:00, 14.63batch/s, val_acc=98.4232%, val_loss=0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Train Loss: 0.0297, Val Acc: 98.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/15: 100%|â–ˆ| 4218/4218 [05:57<00:00, 11.79batch/s, train_acc=98.4528%, train_loss=0\n",
      "Validation Epoch 10/15: 100%|â–ˆ| 528/528 [00:46<00:00, 11.29batch/s, val_acc=98.4173%, val_loss=0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Train Loss: 0.0275, Val Acc: 98.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/15: 100%|â–ˆ| 4218/4218 [05:41<00:00, 12.34batch/s, train_acc=98.4447%, train_loss=0\n",
      "Validation Epoch 11/15: 100%|â–ˆ| 528/528 [00:41<00:00, 12.76batch/s, val_acc=98.4410%, val_loss=0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Train Loss: 0.0276, Val Acc: 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/15: 100%|â–ˆ| 4218/4218 [05:34<00:00, 12.61batch/s, train_acc=98.5521%, train_loss=0\n",
      "Validation Epoch 12/15: 100%|â–ˆ| 528/528 [00:44<00:00, 11.76batch/s, val_acc=98.4766%, val_loss=0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Train Loss: 0.0260, Val Acc: 98.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/15: 100%|â–ˆ| 4218/4218 [06:23<00:00, 11.01batch/s, train_acc=98.5306%, train_loss=0\n",
      "Validation Epoch 13/15: 100%|â–ˆ| 528/528 [00:46<00:00, 11.29batch/s, val_acc=98.4529%, val_loss=0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Train Loss: 0.0260, Val Acc: 98.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/15: 100%|â–ˆ| 4218/4218 [05:46<00:00, 12.18batch/s, train_acc=98.5817%, train_loss=0\n",
      "Validation Epoch 14/15: 100%|â–ˆ| 528/528 [00:43<00:00, 12.16batch/s, val_acc=98.5003%, val_loss=0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Train Loss: 0.0252, Val Acc: 98.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/15: 100%|â–ˆ| 4218/4218 [05:27<00:00, 12.87batch/s, train_acc=98.5981%, train_loss=0\n",
      "Validation Epoch 15/15: 100%|â–ˆ| 528/528 [00:38<00:00, 13.67batch/s, val_acc=98.5122%, val_loss=0.005"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Train Loss: 0.0238, Val Acc: 98.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T11:36:00.250375Z",
     "start_time": "2025-04-19T11:35:11.933466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ðŸ§ª Test accuracy\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "id": "638fdfb707b58d5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.11%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T11:37:38.280263Z",
     "start_time": "2025-04-19T11:37:38.154068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ðŸ’¾ Save the model\n",
    "torch.save(model.state_dict(), 'deepfake_model.pth')\n",
    "print(\"Model saved as deepfake_model.pth\")"
   ],
   "id": "73d44c712fc9cd82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as deepfake_model.pth\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
